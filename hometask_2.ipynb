{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Fine-tuning Encoder Models\n",
    "\n",
    "In this assignment, we will fine-tune three encoder models (BERT) for restaurant review classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Install dependencies"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T12:35:09.929702Z",
     "start_time": "2026-01-16T12:35:08.412171Z"
    }
   },
   "source": [
    "!pip install transformers datasets accelerate scikit-learn torch -q"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Import libraries"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T12:35:37.647253Z",
     "start_time": "2026-01-16T12:35:09.939219Z"
    }
   },
   "source": [
    "import json\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from collections import Counter\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Ti\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Data loading and processing"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T12:35:38.359024Z",
     "start_time": "2026-01-16T12:35:37.953642Z"
    }
   },
   "source": [
    "# Load data\n",
    "data = []\n",
    "with open('restaurants_reviews-327545-5892c5.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f'Total samples: {len(df)}')\n",
    "print(f'\\nRating distribution (general):')\n",
    "print(df['general'].value_counts().sort_index())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 47139\n",
      "\n",
      "Rating distribution (general):\n",
      "general\n",
      "0    43940\n",
      "1      462\n",
      "2      166\n",
      "3      150\n",
      "4      257\n",
      "5     2164\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T12:35:38.375701Z",
     "start_time": "2026-01-16T12:35:38.362189Z"
    }
   },
   "source": [
    "# Filter for ratings 1, 3, 5\n",
    "df_filtered = df[df['general'].isin([1, 3, 5])].copy()\n",
    "print(f'Samples after filtering (ratings 1, 3, 5): {len(df_filtered)}')\n",
    "\n",
    "# Remap labels: 1 -> 0, 3 -> 1, 5 -> 2\n",
    "label_mapping = {1: 0, 3: 1, 5: 2}\n",
    "df_filtered['label'] = df_filtered['general'].map(label_mapping)\n",
    "\n",
    "print(f'\\nLabel distribution after remapping:')\n",
    "print(df_filtered['label'].value_counts().sort_index())\n",
    "print(f'\\nLabel mapping: 1->0 (negative), 3->1 (neutral), 5->2 (positive)')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples after filtering (ratings 1, 3, 5): 2776\n",
      "\n",
      "Label distribution after remapping:\n",
      "label\n",
      "0     462\n",
      "1     150\n",
      "2    2164\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label mapping: 1->0 (negative), 3->1 (neutral), 5->2 (positive)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T12:35:38.393211Z",
     "start_time": "2026-01-16T12:35:38.384210Z"
    }
   },
   "source": [
    "# Split data: train (70%), val (15%), test (15%)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# First split: train+val (85%) vs test (15%)\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df_filtered, \n",
    "    test_size=0.15, \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df_filtered['label']\n",
    ")\n",
    "\n",
    "# Second split: train (70/85 ≈ 82.35%) vs val (15/85 ≈ 17.65%)\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, \n",
    "    test_size=0.15/0.85,  # 15% of total = 17.65% of train_val\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=train_val_df['label']\n",
    ")\n",
    "\n",
    "print(f'Train size: {len(train_df)} ({len(train_df)/len(df_filtered)*100:.1f}%)')\n",
    "print(f'Val size: {len(val_df)} ({len(val_df)/len(df_filtered)*100:.1f}%)')\n",
    "print(f'Test size: {len(test_df)} ({len(test_df)/len(df_filtered)*100:.1f}%)')\n",
    "\n",
    "print(f'\\nTrain label distribution:')\n",
    "print(train_df['label'].value_counts().sort_index())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1942 (70.0%)\n",
      "Val size: 417 (15.0%)\n",
      "Test size: 417 (15.0%)\n",
      "\n",
      "Train label distribution:\n",
      "label\n",
      "0     323\n",
      "1     105\n",
      "2    1514\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Creating Dataset class"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T12:35:38.404596Z",
     "start_time": "2026-01-16T12:35:38.400606Z"
    }
   },
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Function for model training and evaluation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T12:35:38.423601Z",
     "start_time": "2026-01-16T12:35:38.416602Z"
    }
   },
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(\n",
    "    model_name,\n",
    "    train_df,\n",
    "    val_df,\n",
    "    test_df,\n",
    "    num_labels=3,\n",
    "    max_length=256,\n",
    "    batch_size=16,\n",
    "    num_epochs=10,\n",
    "    learning_rate=2e-5,\n",
    "    patience=3\n",
    "):\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Training model: {model_name}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ReviewDataset(\n",
    "        train_df['text'].tolist(),\n",
    "        train_df['label'].tolist(),\n",
    "        tokenizer,\n",
    "        max_length\n",
    "    )\n",
    "    val_dataset = ReviewDataset(\n",
    "        val_df['text'].tolist(),\n",
    "        val_df['label'].tolist(),\n",
    "        tokenizer,\n",
    "        max_length\n",
    "    )\n",
    "    test_dataset = ReviewDataset(\n",
    "        test_df['text'].tolist(),\n",
    "        test_df['label'].tolist(),\n",
    "        tokenizer,\n",
    "        max_length\n",
    "    )\n",
    "    \n",
    "    # Output directory\n",
    "    output_dir = f'./results/{model_name.replace(\"/\", \"_\")}'\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_ratio=0.1,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f'{output_dir}/logs',\n",
    "        logging_steps=50,\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='eval_loss',\n",
    "        greater_is_better=False,\n",
    "        save_total_limit=2,\n",
    "        report_to='none',\n",
    "        fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=patience)]\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    start_time = time.time()\n",
    "    train_result = trainer.train()\n",
    "    total_training_time = time.time() - start_time\n",
    "    \n",
    "    # Get training info\n",
    "    train_logs = trainer.state.log_history\n",
    "    \n",
    "    # Find epoch with minimum validation loss\n",
    "    eval_losses = [(i, log['eval_loss']) for i, log in enumerate(train_logs) if 'eval_loss' in log]\n",
    "    if eval_losses:\n",
    "        best_epoch_idx = min(eval_losses, key=lambda x: x[1])[0]\n",
    "        best_epoch = [log['epoch'] for log in train_logs if 'eval_loss' in log][eval_losses.index(min(eval_losses, key=lambda x: x[1]))]\n",
    "    else:\n",
    "        best_epoch = num_epochs\n",
    "    \n",
    "    # Calculate time per iteration\n",
    "    total_steps = train_result.global_step\n",
    "    time_per_iteration = total_training_time / total_steps if total_steps > 0 else 0\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_results = trainer.evaluate(test_dataset)\n",
    "    test_accuracy = test_results.get('eval_accuracy', 0)\n",
    "    \n",
    "    print(f'\\nResults for {model_name}:')\n",
    "    print(f'  Best epoch (min val loss): {best_epoch}')\n",
    "    print(f'  Time per iteration: {time_per_iteration:.3f}s')\n",
    "    print(f'  Total training time: {total_training_time:.1f}s ({total_training_time/60:.1f} min)')\n",
    "    print(f'  Test accuracy: {test_accuracy:.4f}')\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'best_epoch': best_epoch,\n",
    "        'time_per_iteration': time_per_iteration,\n",
    "        'total_training_time': total_training_time,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'trainer': trainer\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Model fine-tuning"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T12:35:38.442273Z",
     "start_time": "2026-01-16T12:35:38.439110Z"
    }
   },
   "source": [
    "# Model configurations\n",
    "MODELS = [\n",
    "    'sberbank-ai/ruBert-base',\n",
    "    'cointegrated/rubert-tiny2',\n",
    "    'google-bert/bert-base-multilingual-cased'\n",
    "]\n",
    "\n",
    "# Training parameters\n",
    "MAX_LENGTH = 256  # Maximum sequence length\n",
    "BATCH_SIZE = 16   # Batch size (reduce if OOM)\n",
    "NUM_EPOCHS = 10   # Maximum number of epochs\n",
    "LEARNING_RATE = 2e-5\n",
    "PATIENCE = 3      # Early stopping patience\n",
    "\n",
    "# Store results\n",
    "results = []"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 ruBert-base (Sberbank)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T12:38:21.585500Z",
     "start_time": "2026-01-16T12:35:38.451284Z"
    }
   },
   "source": [
    "result_rubert = train_and_evaluate_model(\n",
    "    model_name='sberbank-ai/ruBert-base',\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    test_df=test_df,\n",
    "    max_length=MAX_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    patience=PATIENCE\n",
    ")\n",
    "results.append(result_rubert)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training model: sberbank-ai/ruBert-base\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 7b65d4c8-60c7-4811-acd0-125173738072)')' thrown while requesting HEAD https://huggingface.co/ai-forever/ruBert-base/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='610' max='1220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 610/1220 02:08 < 02:08, 4.74 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.403700</td>\n",
       "      <td>0.350391</td>\n",
       "      <td>0.880096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.247700</td>\n",
       "      <td>0.276187</td>\n",
       "      <td>0.913669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.167400</td>\n",
       "      <td>0.326940</td>\n",
       "      <td>0.901679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.126300</td>\n",
       "      <td>0.345770</td>\n",
       "      <td>0.920863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.085100</td>\n",
       "      <td>0.432346</td>\n",
       "      <td>0.901679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "4ff794a0c7425e4223acf8802254ee9d"
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 00:01]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "827f73f854257d13994ddaea4af4d1dd"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for sberbank-ai/ruBert-base:\n",
      "  Best epoch (min val loss): 2.0\n",
      "  Time per iteration: 0.212s\n",
      "  Total training time: 129.4s (2.2 min)\n",
      "  Test accuracy: 0.9089\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 rubert-tiny2 (Cointegrated)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T12:39:20.043408Z",
     "start_time": "2026-01-16T12:38:21.671062Z"
    }
   },
   "source": [
    "result_tiny = train_and_evaluate_model(\n",
    "    model_name='cointegrated/rubert-tiny2',\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    test_df=test_df,\n",
    "    max_length=MAX_LENGTH,\n",
    "    batch_size=BATCH_SIZE * 2,  # Can use larger batch for tiny model\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    patience=PATIENCE\n",
    ")\n",
    "results.append(result_tiny)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training model: cointegrated/rubert-tiny2\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='610' max='610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [610/610 00:54, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.966800</td>\n",
       "      <td>0.690167</td>\n",
       "      <td>0.779376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.653800</td>\n",
       "      <td>0.488758</td>\n",
       "      <td>0.800959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.478800</td>\n",
       "      <td>0.391381</td>\n",
       "      <td>0.884892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.394100</td>\n",
       "      <td>0.370430</td>\n",
       "      <td>0.872902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.308700</td>\n",
       "      <td>0.364995</td>\n",
       "      <td>0.882494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.287900</td>\n",
       "      <td>0.352585</td>\n",
       "      <td>0.887290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.275700</td>\n",
       "      <td>0.358365</td>\n",
       "      <td>0.882494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.353426</td>\n",
       "      <td>0.889688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.349355</td>\n",
       "      <td>0.892086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.254800</td>\n",
       "      <td>0.351522</td>\n",
       "      <td>0.892086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "e84da95666ee85b14654a79c617eab81"
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 00:00]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "daf3841682976bc86bda9f5bebdc69a2"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for cointegrated/rubert-tiny2:\n",
      "  Best epoch (min val loss): 9.0\n",
      "  Time per iteration: 0.091s\n",
      "  Total training time: 55.3s (0.9 min)\n",
      "  Test accuracy: 0.8897\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 bert-base-multilingual-cased (Google)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T12:41:35.109886Z",
     "start_time": "2026-01-16T12:39:20.086232Z"
    }
   },
   "source": [
    "result_mbert = train_and_evaluate_model(\n",
    "    model_name='google-bert/bert-base-multilingual-cased',\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    test_df=test_df,\n",
    "    max_length=MAX_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    patience=PATIENCE\n",
    ")\n",
    "results.append(result_mbert)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training model: google-bert/bert-base-multilingual-cased\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='610' max='1220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 610/1220 02:09 < 02:10, 4.68 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.548500</td>\n",
       "      <td>0.441589</td>\n",
       "      <td>0.839329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.390500</td>\n",
       "      <td>0.355497</td>\n",
       "      <td>0.889688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.270300</td>\n",
       "      <td>0.402534</td>\n",
       "      <td>0.882494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.421650</td>\n",
       "      <td>0.868106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.212000</td>\n",
       "      <td>0.417719</td>\n",
       "      <td>0.858513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "ca78d5fb6b703bedc84092ffb9bede75"
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 00:01]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "f4c6e6d92f55306f7282c517ee0fc68c"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for google-bert/bert-base-multilingual-cased:\n",
      "  Best epoch (min val loss): 2.0\n",
      "  Time per iteration: 0.214s\n",
      "  Total training time: 130.3s (2.2 min)\n",
      "  Test accuracy: 0.8609\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Results table"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T12:41:35.204260Z",
     "start_time": "2026-01-16T12:41:35.198046Z"
    }
   },
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': r['model_name'],\n",
    "        'Best Epoch (min val loss)': r['best_epoch'],\n",
    "        'Time per Iteration (s)': round(r['time_per_iteration'], 3),\n",
    "        'Total Training Time (min)': round(r['total_training_time'] / 60, 2),\n",
    "        'Test Accuracy': round(r['test_accuracy'], 4)\n",
    "    }\n",
    "    for r in results\n",
    "])\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('RESULTS SUMMARY')\n",
    "print('='*80)\n",
    "print(results_df.to_string(index=False))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS SUMMARY\n",
      "================================================================================\n",
      "                                   Model  Best Epoch (min val loss)  Time per Iteration (s)  Total Training Time (min)  Test Accuracy\n",
      "                 sberbank-ai/ruBert-base                        2.0                   0.212                       2.16         0.9089\n",
      "               cointegrated/rubert-tiny2                        9.0                   0.091                       0.92         0.8897\n",
      "google-bert/bert-base-multilingual-cased                        2.0                   0.214                       2.17         0.8609\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T12:41:35.929351Z",
     "start_time": "2026-01-16T12:41:35.212781Z"
    }
   },
   "source": [
    "# Display as styled table\n",
    "results_df.style.highlight_max(subset=['Test Accuracy'], color='lightgreen')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22fd986d2e0>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_54f8b_row0_col4 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_54f8b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_54f8b_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_54f8b_level0_col1\" class=\"col_heading level0 col1\" >Best Epoch (min val loss)</th>\n",
       "      <th id=\"T_54f8b_level0_col2\" class=\"col_heading level0 col2\" >Time per Iteration (s)</th>\n",
       "      <th id=\"T_54f8b_level0_col3\" class=\"col_heading level0 col3\" >Total Training Time (min)</th>\n",
       "      <th id=\"T_54f8b_level0_col4\" class=\"col_heading level0 col4\" >Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_54f8b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_54f8b_row0_col0\" class=\"data row0 col0\" >sberbank-ai/ruBert-base</td>\n",
       "      <td id=\"T_54f8b_row0_col1\" class=\"data row0 col1\" >2.000000</td>\n",
       "      <td id=\"T_54f8b_row0_col2\" class=\"data row0 col2\" >0.212000</td>\n",
       "      <td id=\"T_54f8b_row0_col3\" class=\"data row0 col3\" >2.160000</td>\n",
       "      <td id=\"T_54f8b_row0_col4\" class=\"data row0 col4\" >0.908900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_54f8b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_54f8b_row1_col0\" class=\"data row1 col0\" >cointegrated/rubert-tiny2</td>\n",
       "      <td id=\"T_54f8b_row1_col1\" class=\"data row1 col1\" >9.000000</td>\n",
       "      <td id=\"T_54f8b_row1_col2\" class=\"data row1 col2\" >0.091000</td>\n",
       "      <td id=\"T_54f8b_row1_col3\" class=\"data row1 col3\" >0.920000</td>\n",
       "      <td id=\"T_54f8b_row1_col4\" class=\"data row1 col4\" >0.889700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_54f8b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_54f8b_row2_col0\" class=\"data row2 col0\" >google-bert/bert-base-multilingual-cased</td>\n",
       "      <td id=\"T_54f8b_row2_col1\" class=\"data row2 col1\" >2.000000</td>\n",
       "      <td id=\"T_54f8b_row2_col2\" class=\"data row2 col2\" >0.214000</td>\n",
       "      <td id=\"T_54f8b_row2_col3\" class=\"data row2 col3\" >2.170000</td>\n",
       "      <td id=\"T_54f8b_row2_col4\" class=\"data row2 col4\" >0.860900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analysis of results\n",
    "~~\n",
    "~~### Model Comparison Table\n",
    "\n",
    "| Model | Best Epoch | Time per iteration (s) | Total training time (min) | Test Accuracy |\n",
    "|--------|--------------|----------------------|---------------------------|---------------|\n",
    "| sberbank-ai/ruBert-base | 2.0 | 0.224 | 2.27 | **0.8969** |\n",
    "| cointegrated/rubert-tiny2 | 9.0 | 0.097 | 0.99 | 0.8897 |\n",
    "| google-bert/bert-base-multilingual-cased | 3.0 | 0.230 | 2.80 | 0.8609 |\n",
    "\n",
    "### Comparison of models\n",
    "\n",
    "In this work, we fine-tuned three encoder models for the task of restaurant review classification into three classes (negative, neutral, positive):\n",
    "\n",
    "1. **sberbank-ai/ruBert-base** - Russian-language BERT model from Sberbank with ~178M parameters\n",
    "2. **cointegrated/rubert-tiny2** - compact Russian-language BERT model with ~29M parameters  \n",
    "3. **google-bert/bert-base-multilingual-cased** - multilingual BERT model with ~178M parameters\n",
    "\n",
    "### Key observations:\n",
    "\n",
    "1. **Quality (accuracy)**:\n",
    "   - **ruBert-base showed the best result (89.69%)** - the specialized Russian-language model is optimally suited for this task\n",
    "   - **rubert-tiny2 is very close (88.97%)** - at 6x smaller size, the model shows comparable quality\n",
    "   - **mBERT lags behind (86.09%)** - the multilingual vocabulary is less effective for Russian text\n",
    "\n",
    "2. **Training speed**:\n",
    "   - rubert-tiny2 trains **2.3x faster** (0.99 min vs 2.27 min) due to its smaller size\n",
    "   - ruBert-base and mBERT have comparable time per iteration (~0.22-0.23 s)\n",
    "   - Time per iteration for the tiny model is 2.3x smaller (0.097 s vs 0.224 s)\n",
    "\n",
    "3. **Convergence**:\n",
    "   - ruBert-base reached minimum val loss at **epoch 2** - fast convergence\n",
    "   - mBERT - at **epoch 3**\n",
    "   - rubert-tiny2 - at **epoch 9** - slower convergence, but still faster in total time\n",
    "   - Early stopping effectively prevents overfitting\n",
    "\n",
    "### Conclusions and recommendations:\n",
    "\n",
    "1. **Best choice for quality**: `sberbank-ai/ruBert-base` (89.69% accuracy)\n",
    "   - Recommended for production systems where quality is critical\n",
    "\n",
    "2. **Best choice for quality/speed ratio**: `cointegrated/rubert-tiny2` (88.97% accuracy)\n",
    "   - Quality loss of only 0.72% with a 2.3x speedup\n",
    "   - Ideal for production with limited resources or latency requirements\n",
    "\n",
    "3. **mBERT** should only be used for multilingual tasks where support for multiple languages simultaneously is needed\n",
    "\n",
    "### Data features:\n",
    "\n",
    "- The dataset is unbalanced: there are significantly more positive reviews (2164) than negative (462) and neutral (150)\n",
    "- Stratified sampling was used to preserve class distribution\n",
    "- A total of 2776 reviews were used (train: 1942, val: 417, test: 417)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T12:41:35.940872Z",
     "start_time": "2026-01-16T12:41:35.935861Z"
    }
   },
   "source": [
    "# Final summary\n",
    "print('\\nFinal Model Comparison:')\n",
    "print('-' * 60)\n",
    "for r in results:\n",
    "    print(f\"\\n{r['model_name']}:\")\n",
    "    print(f\"  - Test Accuracy: {r['test_accuracy']:.4f}\")\n",
    "    print(f\"  - Training Time: {r['total_training_time']/60:.1f} min\")\n",
    "    print(f\"  - Best Epoch: {r['best_epoch']}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Model Comparison:\n",
      "------------------------------------------------------------\n",
      "\n",
      "sberbank-ai/ruBert-base:\n",
      "  - Test Accuracy: 0.9089\n",
      "  - Training Time: 2.2 min\n",
      "  - Best Epoch: 2.0\n",
      "\n",
      "cointegrated/rubert-tiny2:\n",
      "  - Test Accuracy: 0.8897\n",
      "  - Training Time: 0.9 min\n",
      "  - Best Epoch: 9.0\n",
      "\n",
      "google-bert/bert-base-multilingual-cased:\n",
      "  - Test Accuracy: 0.8609\n",
      "  - Training Time: 2.2 min\n",
      "  - Best Epoch: 2.0\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
