{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_md",
   "metadata": {},
   "source": [
    "# Homework 4: Prompting Language Models\n",
    "\n",
    "In this assignment, we work with pre-trained large language models (LLMs) to solve the task of determining linguistic acceptability of Russian sentences (RuCoLA dataset).\n",
    "\n",
    "**Tasks:**\n",
    "1. Load RuCoLA data (in_domain_dev.csv)\n",
    "2. Use Qwen2.5-1.5B-Instruct model with various prompts\n",
    "3. Experiment with prompts in Russian and English languages\n",
    "4. Add System Prompt and conduct similar experiments\n",
    "5. Compare with base model (without Instruct)\n",
    "6. Draw conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_md",
   "metadata": {},
   "source": [
    "## 1. Installation and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "install_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T07:39:13.659227Z",
     "start_time": "2026-01-17T07:39:12.104448Z"
    }
   },
   "source": [
    "# Install required libraries\n",
    "!pip install transformers torch pandas scikit-learn tqdm accelerate -q"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "imports_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T07:39:13.672611Z",
     "start_time": "2026-01-17T07:39:13.668277Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Ti\n",
      "Memory: 17.18 GB\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "data_md",
   "metadata": {},
   "source": [
    "## 2. Loading RuCoLA Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "load_data_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T07:39:13.923058Z",
     "start_time": "2026-01-17T07:39:13.702660Z"
    }
   },
   "source": "# Load data\ndata_url = \"https://raw.githubusercontent.com/RussianNLP/RuCoLA/main/data/in_domain_dev.csv\"\ndf = pd.read_csv(data_url)\n\nprint(f\"Dataset size: {len(df)}\")\nprint(f\"\\nColumns: {df.columns.tolist()}\")\nprint(f\"\\nLabel distribution:\")\nprint(df['acceptable'].value_counts())\nprint(f\"\\nData examples:\")\ndf.head(10)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 983\n",
      "\n",
      "Columns: ['id', 'sentence', 'acceptable', 'error_type', 'detailed_source']\n",
      "\n",
      "Label distribution:\n",
      "acceptable\n",
      "1    733\n",
      "0    250\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data examples:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   id                                           sentence  acceptable  \\\n",
       "0   0                            Иван вчера не позвонил.           1   \n",
       "1   1  У многих туристов, кто посещают Кемер весной, ...           0   \n",
       "2   2  Лесные запахи набегали волнами; в них смешалос...           1   \n",
       "3   3  Вчера президент имел неофициальную беседу с ан...           1   \n",
       "4   4  Коллега так и не признал вину за катастрофу пе...           1   \n",
       "5   5                   Я говорил с ним только ради Вас.           1   \n",
       "6   6  Этот игрок был куплен «Реалом», чтобы он играл...           1   \n",
       "7   7       Ивану удалось попасть на концерт Макаревича.           1   \n",
       "8   8              Ты посылал ей приглашение на свадьбу?           1   \n",
       "9   9  После счастливого конца Тюлин предложил зайти ...           1   \n",
       "\n",
       "  error_type detailed_source  \n",
       "0          0   Paducheva2013  \n",
       "1     Syntax            USE8  \n",
       "2          0            USE5  \n",
       "3          0    Seliverstova  \n",
       "4          0       Testelets  \n",
       "5          0    Seliverstova  \n",
       "6          0       Testelets  \n",
       "7          0   Paducheva2013  \n",
       "8          0   Paducheva2010  \n",
       "9          0       Testelets  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>error_type</th>\n",
       "      <th>detailed_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Иван вчера не позвонил.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>У многих туристов, кто посещают Кемер весной, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Syntax</td>\n",
       "      <td>USE8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Лесные запахи набегали волнами; в них смешалос...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>USE5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Вчера президент имел неофициальную беседу с ан...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Seliverstova</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Коллега так и не признал вину за катастрофу пе...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Testelets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Я говорил с ним только ради Вас.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Seliverstova</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Этот игрок был куплен «Реалом», чтобы он играл...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Testelets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Ивану удалось попасть на концерт Макаревича.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Ты посылал ей приглашение на свадьбу?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>После счастливого конца Тюлин предложил зайти ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Testelets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "explore_data_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T07:39:13.941592Z",
     "start_time": "2026-01-17T07:39:13.935079Z"
    }
   },
   "source": "# Examples of acceptable and unacceptable sentences\nprint(\"Examples of ACCEPTABLE sentences (acceptable=1):\")\nfor i, row in df[df['acceptable'] == 1].head(3).iterrows():\n    print(f\"  - {row['sentence']}\")\n\nprint(\"\\nExamples of UNACCEPTABLE sentences (acceptable=0):\")\nfor i, row in df[df['acceptable'] == 0].head(3).iterrows():\n    print(f\"  - {row['sentence']}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of ACCEPTABLE sentences (acceptable=1):\n",
      "  - Иван вчера не позвонил.\n",
      "  - Лесные запахи набегали волнами; в них смешалось дыхание можжевельника, вереска, брусники.\n",
      "  - Вчера президент имел неофициальную беседу с английским послом.\n",
      "\n",
      "Examples of UNACCEPTABLE sentences (acceptable=0):\n",
      "  - У многих туристов, кто посещают Кемер весной, есть шанс застать снег на вершине горы Тахталы и даже сочетать пляжный отдых с горнолыжным.\n",
      "  - Вчера в два часа магазин закрыт.\n",
      "  - А ты ехай прямо к директору театров, князю Гагарину.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "model_md",
   "metadata": {},
   "source": [
    "## 3. Loading Qwen2.5-1.5B-Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "load_model_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T07:44:56.447861Z",
     "start_time": "2026-01-17T07:39:13.949105Z"
    }
   },
   "source": [
    "def load_model(model_name, is_instruct=True):\n",
    "    \"\"\"Load model and tokenizer\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if device == \"cuda\" else None,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    if device == \"cpu\":\n",
    "        model = model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    print(f\"Model loaded on {device}\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# Load Instruct model\n",
    "model_instruct, tokenizer_instruct = load_model(\"Qwen/Qwen2.5-1.5B-Instruct\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Qwen/Qwen2.5-1.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "postprocess_md",
   "metadata": {},
   "source": [
    "## 4. Post-processing Function for Model Responses"
   ]
  },
  {
   "cell_type": "code",
   "id": "postprocess_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T07:44:56.577126Z",
     "start_time": "2026-01-17T07:44:56.563588Z"
    }
   },
   "source": [
    "def extract_label(response, verbose=False):\n",
    "    \"\"\"\n",
    "    Extract label from model response.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Look for explicit numeric answers (0 or 1)\n",
    "    2. Look for keywords indicating acceptability\n",
    "    3. Look for keywords indicating unacceptability\n",
    "    4. Return -1 by default (could not determine)\n",
    "    \"\"\"\n",
    "    response_lower = response.lower().strip()\n",
    "    \n",
    "    # Patterns for numeric answers\n",
    "    # Look at the beginning of the response\n",
    "    if response_lower.startswith('1') or response_lower.startswith('«1»'):\n",
    "        return 1\n",
    "    if response_lower.startswith('0') or response_lower.startswith('«0»'):\n",
    "        return 0\n",
    "    \n",
    "    # Look for patterns like \"answer: 1\" or \"label: 0\"\n",
    "    patterns_1 = [\n",
    "        r'\\b(?:ответ|answer|label|метка|результат|result)[:\\s]*1\\b',\n",
    "        r'\\b1\\b.*(?:приемлем|acceptable|correct|верн|правильн)',\n",
    "        r'(?:приемлем|acceptable|correct|верн|правильн).*\\b1\\b',\n",
    "    ]\n",
    "    patterns_0 = [\n",
    "        r'\\b(?:ответ|answer|label|метка|результат|result)[:\\s]*0\\b',\n",
    "        r'\\b0\\b.*(?:неприемлем|unacceptable|incorrect|невер|неправильн)',\n",
    "        r'(?:неприемлем|unacceptable|incorrect|невер|неправильн).*\\b0\\b',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns_1:\n",
    "        if re.search(pattern, response_lower):\n",
    "            return 1\n",
    "    for pattern in patterns_0:\n",
    "        if re.search(pattern, response_lower):\n",
    "            return 0\n",
    "    \n",
    "    # Keywords for acceptability (check at the beginning of response)\n",
    "    acceptable_keywords = [\n",
    "        'приемлемо', 'приемлем', 'acceptable', 'correct', 'верно', 'верн',\n",
    "        'правильно', 'правильн', 'грамматически верно', 'grammatically correct',\n",
    "        'да,', 'да.', 'yes', 'корректно', 'корректн', 'допустим', 'natural',\n",
    "        'естественно', 'нормально', 'хорошо сформулировано'\n",
    "    ]\n",
    "    \n",
    "    # Keywords for unacceptability\n",
    "    unacceptable_keywords = [\n",
    "        'неприемлемо', 'неприемлем', 'unacceptable', 'incorrect', 'неверно', \n",
    "        'неправильно', 'ошибка', 'error', 'wrong', 'некорректно', 'недопустим',\n",
    "        'нет,', 'нет.', 'no,', 'no.', 'unnatural', 'неестественно', 'странно',\n",
    "        'не является приемлемым', 'is not acceptable', 'грамматическая ошибка'\n",
    "    ]\n",
    "    \n",
    "    # Check first 100 characters to determine the answer\n",
    "    first_part = response_lower[:100]\n",
    "    \n",
    "    # First check for unacceptability (since \"unacceptable\" contains \"acceptable\" in Russian)\n",
    "    for keyword in unacceptable_keywords:\n",
    "        if keyword in first_part:\n",
    "            return 0\n",
    "    \n",
    "    for keyword in acceptable_keywords:\n",
    "        if keyword in first_part:\n",
    "            return 1\n",
    "    \n",
    "    # Check the entire response\n",
    "    for keyword in unacceptable_keywords:\n",
    "        if keyword in response_lower:\n",
    "            return 0\n",
    "    \n",
    "    for keyword in acceptable_keywords:\n",
    "        if keyword in response_lower:\n",
    "            return 1\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Could not determine label for response: {response[:100]}...\")\n",
    "    \n",
    "    return -1  # Could not determine\n",
    "\n",
    "\n",
    "# Test the post-processing function\n",
    "test_responses = [\n",
    "    \"1\",\n",
    "    \"0\",\n",
    "    \"Answer: 1\",\n",
    "    \"The sentence is acceptable.\",\n",
    "    \"This sentence is unacceptable.\",\n",
    "    \"Yes, this is a grammatically correct sentence.\",\n",
    "    \"No, there is an error in the sentence.\",\n",
    "    \"Unacceptable. The word order is violated.\"\n",
    "]\n",
    "\n",
    "print(\"Testing extract_label function:\")\n",
    "for resp in test_responses:\n",
    "    label = extract_label(resp)\n",
    "    print(f\"  '{resp[:50]}...' -> {label}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing extract_label function:\n",
      "  '1...' -> 1\n",
      "  '0...' -> 0\n",
      "  'Answer: 1...' -> 1\n",
      "  'The sentence is acceptable....' -> 1\n",
      "  'This sentence is unacceptable....' -> 0\n",
      "  'Yes, this is a grammatically correct sentence....' -> 1\n",
      "  'No, there is an error in the sentence....' -> 0\n",
      "  'Unacceptable. The word order is violated....' -> 0\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "inference_md",
   "metadata": {},
   "source": [
    "## 5. Generation and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "inference_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T07:44:56.643252Z",
     "start_time": "2026-01-17T07:44:56.628764Z"
    }
   },
   "source": "def generate_response(model, tokenizer, prompt, system_prompt=None, max_new_tokens=50):\n    \"\"\"\n    Generate model response.\n    Uses chat template for Instruct models.\n    \"\"\"\n    messages = []\n    \n    if system_prompt:\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\n    \n    messages.append({\"role\": \"user\", \"content\": prompt})\n    \n    # Use chat template if available\n    if hasattr(tokenizer, 'apply_chat_template'):\n        text = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n    else:\n        # Fallback for models without chat template\n        if system_prompt:\n            text = f\"{system_prompt}\\n\\nUser: {prompt}\\nAssistant:\"\n        else:\n            text = f\"User: {prompt}\\nAssistant:\"\n    \n    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=False,  # Deterministic generation\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode only new tokens\n    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n    return response.strip()\n\n\ndef generate_response_base(model, tokenizer, prompt, max_new_tokens=50):\n    \"\"\"\n    Generate response for base (non-Instruct) model.\n    Uses completion-style prompting.\n    \"\"\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=False,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n    return response.strip()\n\n\ndef evaluate_model(model, tokenizer, df, prompt_template, system_prompt=None, \n                   is_instruct=True, max_samples=None, verbose=True):\n    \"\"\"\n    Evaluate model on dataset.\n    \n    Args:\n        model: the model\n        tokenizer: the tokenizer\n        df: DataFrame with data\n        prompt_template: prompt template (must contain {sentence})\n        system_prompt: system prompt (optional)\n        is_instruct: whether the model is an Instruct version\n        max_samples: maximum number of samples (None = all)\n        verbose: print progress\n    \n    Returns:\n        dict with metrics and predictions\n    \"\"\"\n    if max_samples:\n        data = df.head(max_samples)\n    else:\n        data = df\n    \n    predictions = []\n    responses = []\n    true_labels = data['acceptable'].tolist()\n    \n    iterator = tqdm(data.iterrows(), total=len(data), disable=not verbose)\n    \n    for idx, row in iterator:\n        sentence = row['sentence']\n        prompt = prompt_template.format(sentence=sentence)\n        \n        if is_instruct:\n            response = generate_response(model, tokenizer, prompt, system_prompt)\n        else:\n            response = generate_response_base(model, tokenizer, prompt)\n        \n        label = extract_label(response)\n        \n        predictions.append(label)\n        responses.append(response)\n    \n    # Handle undefined labels\n    # Replace -1 with the most frequent class for metric calculation\n    predictions_clean = [p if p != -1 else 1 for p in predictions]  # default to 1 (acceptable)\n    \n    # Calculate metrics\n    accuracy = accuracy_score(true_labels, predictions_clean)\n    f1 = f1_score(true_labels, predictions_clean, average='macro')\n    \n    # Count undefined responses\n    undefined_count = predictions.count(-1)\n    \n    results = {\n        'accuracy': accuracy,\n        'f1_macro': f1,\n        'undefined_count': undefined_count,\n        'undefined_ratio': undefined_count / len(predictions),\n        'predictions': predictions,\n        'predictions_clean': predictions_clean,\n        'responses': responses,\n        'true_labels': true_labels\n    }\n    \n    if verbose:\n        print(f\"\\nResults:\")\n        print(f\"  Accuracy: {accuracy:.4f}\")\n        print(f\"  F1 (macro): {f1:.4f}\")\n        print(f\"  Undefined responses: {undefined_count} ({undefined_count/len(predictions)*100:.1f}%)\")\n    \n    return results",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "exp1_md",
   "metadata": {},
   "source": [
    "## 6. Experiment 1: Prompts without System Prompt\n",
    "\n",
    "Testing various prompt formulations in Russian and English languages."
   ]
  },
  {
   "cell_type": "code",
   "id": "prompts_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T07:44:56.678390Z",
     "start_time": "2026-01-17T07:44:56.670828Z"
    }
   },
   "source": [
    "# Define prompts for experiments\n",
    "\n",
    "prompts_ru = {\n",
    "    \"simple_ru\": \"\"\"Определи, является ли следующее предложение на русском языке приемлемым (грамматически корректным и естественным).\n",
    "\n",
    "Предложение: {sentence}\n",
    "\n",
    "Ответь только 1 (приемлемо) или 0 (неприемлемо).\"\"\",\n",
    "\n",
    "    \"detailed_ru\": \"\"\"Задача: определить лингвистическую приемлемость предложения на русском языке.\n",
    "\n",
    "Приемлемое предложение - это предложение, которое носитель русского языка воспринял бы как естественное и грамматически корректное.\n",
    "\n",
    "Предложение для анализа: {sentence}\n",
    "\n",
    "Является ли это предложение приемлемым? Ответь 1 (да) или 0 (нет).\"\"\",\n",
    "\n",
    "    \"cot_ru\": \"\"\"Проанализируй следующее предложение на русском языке на предмет грамматической корректности и естественности.\n",
    "\n",
    "Предложение: {sentence}\n",
    "\n",
    "Сначала кратко объясни, есть ли в предложении ошибки, затем дай окончательный ответ: 1 (приемлемо) или 0 (неприемлемо).\"\"\"\n",
    "}\n",
    "\n",
    "prompts_en = {\n",
    "    \"simple_en\": \"\"\"Determine if the following Russian sentence is acceptable (grammatically correct and natural).\n",
    "\n",
    "Sentence: {sentence}\n",
    "\n",
    "Answer only 1 (acceptable) or 0 (unacceptable).\"\"\",\n",
    "\n",
    "    \"detailed_en\": \"\"\"Task: Determine the linguistic acceptability of a Russian sentence.\n",
    "\n",
    "An acceptable sentence is one that a native Russian speaker would perceive as natural and grammatically correct.\n",
    "\n",
    "Sentence to analyze: {sentence}\n",
    "\n",
    "Is this sentence acceptable? Answer 1 (yes) or 0 (no).\"\"\",\n",
    "\n",
    "    \"cot_en\": \"\"\"Analyze the following Russian sentence for grammatical correctness and naturalness.\n",
    "\n",
    "Sentence: {sentence}\n",
    "\n",
    "First briefly explain if there are any errors, then give your final answer: 1 (acceptable) or 0 (unacceptable).\"\"\"\n",
    "}\n",
    "\n",
    "all_prompts_no_system = {**prompts_ru, **prompts_en}\n",
    "\n",
    "print(\"Defined prompts:\", len(all_prompts_no_system))\n",
    "for name in all_prompts_no_system:\n",
    "    print(f\"  - {name}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined prompts: 6\n",
      "  - simple_ru\n",
      "  - detailed_ru\n",
      "  - cot_ru\n",
      "  - simple_en\n",
      "  - detailed_en\n",
      "  - cot_en\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "exp1_run_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T09:08:07.504042Z",
     "start_time": "2026-01-17T07:44:56.708447Z"
    }
   },
   "source": [
    "# Run experiments without System Prompt\n",
    "# Using a subset of data for quick testing (can be increased)\n",
    "\n",
    "MAX_SAMPLES = 200  # Set to None to use the entire dataset\n",
    "\n",
    "results_no_system = {}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 1: Prompts without System Prompt\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for prompt_name, prompt_template in all_prompts_no_system.items():\n",
    "    print(f\"\\n--- Testing: {prompt_name} ---\")\n",
    "    \n",
    "    results = evaluate_model(\n",
    "        model_instruct, \n",
    "        tokenizer_instruct, \n",
    "        df, \n",
    "        prompt_template,\n",
    "        system_prompt=None,\n",
    "        is_instruct=True,\n",
    "        max_samples=MAX_SAMPLES\n",
    "    )\n",
    "    \n",
    "    results_no_system[prompt_name] = results"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPERIMENT 1: Prompts without System Prompt\n",
      "============================================================\n",
      "\n",
      "--- Testing: simple_ru ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "100%|██████████| 200/200 [03:55<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy: 0.7350\n",
      "  F1 (macro): 0.5007\n",
      "  Undefined responses: 0 (0.0%)\n",
      "\n",
      "--- Testing: detailed_ru ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [11:33<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy: 0.7100\n",
      "  F1 (macro): 0.5289\n",
      "  Undefined responses: 0 (0.0%)\n",
      "\n",
      "--- Testing: cot_ru ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [59:48<00:00, 17.94s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy: 0.7050\n",
      "  F1 (macro): 0.4442\n",
      "  Undefined responses: 37 (18.5%)\n",
      "\n",
      "--- Testing: simple_en ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:18<00:00, 10.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy: 0.6550\n",
      "  F1 (macro): 0.5014\n",
      "  Undefined responses: 0 (0.0%)\n",
      "\n",
      "--- Testing: detailed_en ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:33<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy: 0.7000\n",
      "  F1 (macro): 0.5026\n",
      "  Undefined responses: 0 (0.0%)\n",
      "\n",
      "--- Testing: cot_en ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [07:01<00:00,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy: 0.6600\n",
      "  F1 (macro): 0.4241\n",
      "  Undefined responses: 25 (12.5%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "exp1_summary_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T09:08:07.575822Z",
     "start_time": "2026-01-17T09:08:07.564763Z"
    }
   },
   "source": [
    "# Summary of Experiment 1 results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 1 SUMMARY: Without System Prompt\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_no_system = []\n",
    "for name, res in results_no_system.items():\n",
    "    summary_no_system.append({\n",
    "        'Prompt': name,\n",
    "        'Accuracy': f\"{res['accuracy']:.4f}\",\n",
    "        'F1 (macro)': f\"{res['f1_macro']:.4f}\",\n",
    "        'Undefined (%)': f\"{res['undefined_ratio']*100:.1f}%\"\n",
    "    })\n",
    "\n",
    "summary_df_1 = pd.DataFrame(summary_no_system)\n",
    "print(summary_df_1.to_string(index=False))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXPERIMENT 1 SUMMARY: Without System Prompt\n",
      "============================================================\n",
      "     Prompt Accuracy F1 (macro) Undefined (%)\n",
      "  simple_ru   0.7350     0.5007          0.0%\n",
      "detailed_ru   0.7100     0.5289          0.0%\n",
      "     cot_ru   0.7050     0.4442         18.5%\n",
      "  simple_en   0.6550     0.5014          0.0%\n",
      "detailed_en   0.7000     0.5026          0.0%\n",
      "     cot_en   0.6600     0.4241         12.5%\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "exp1_examples_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T09:08:07.602463Z",
     "start_time": "2026-01-17T09:08:07.596331Z"
    }
   },
   "source": [
    "# Examples of model responses\n",
    "best_prompt = max(results_no_system, key=lambda x: results_no_system[x]['accuracy'])\n",
    "print(f\"\\nModel response examples (prompt: {best_prompt}):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "res = results_no_system[best_prompt]\n",
    "for i in range(min(5, len(res['responses']))):\n",
    "    sentence = df.iloc[i]['sentence']\n",
    "    true_label = res['true_labels'][i]\n",
    "    pred_label = res['predictions'][i]\n",
    "    response = res['responses'][i]\n",
    "    \n",
    "    status = \"✓\" if pred_label == true_label else \"✗\"\n",
    "    print(f\"\\n{status} Sentence: {sentence[:60]}...\")\n",
    "    print(f\"   True label: {true_label}, Prediction: {pred_label}\")\n",
    "    print(f\"   Model response: {response[:100]}...\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model response examples (prompt: simple_ru):\n",
      "------------------------------------------------------------\n",
      "\n",
      "✓ Sentence: Иван вчера не позвонил....\n",
      "   True label: 1, Prediction: 1\n",
      "   Model response: 1...\n",
      "\n",
      "✗ Sentence: У многих туристов, кто посещают Кемер весной, есть шанс заст...\n",
      "   True label: 0, Prediction: 1\n",
      "   Model response: 1...\n",
      "\n",
      "✓ Sentence: Лесные запахи набегали волнами; в них смешалось дыхание можж...\n",
      "   True label: 1, Prediction: 1\n",
      "   Model response: 1...\n",
      "\n",
      "✓ Sentence: Вчера президент имел неофициальную беседу с английским посло...\n",
      "   True label: 1, Prediction: 1\n",
      "   Model response: 1...\n",
      "\n",
      "✓ Sentence: Коллега так и не признал вину за катастрофу перед коллективо...\n",
      "   True label: 1, Prediction: 1\n",
      "   Model response: 1...\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "exp2_md",
   "metadata": {},
   "source": [
    "## 7. Experiment 2: Prompts with System Prompt\n",
    "\n",
    "Adding various system prompts and analyzing their impact on quality."
   ]
  },
  {
   "cell_type": "code",
   "id": "system_prompts_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T09:08:07.633309Z",
     "start_time": "2026-01-17T09:08:07.620468Z"
    }
   },
   "source": [
    "# Define system prompts\n",
    "\n",
    "system_prompts = {\n",
    "    \"expert_ru\": \"\"\"Ты эксперт-лингвист, специализирующийся на русском языке. \n",
    "Твоя задача - определять, является ли предложение лингвистически приемлемым.\n",
    "Отвечай кратко и точно.\"\"\",\n",
    "\n",
    "    \"expert_en\": \"\"\"You are an expert linguist specializing in Russian language.\n",
    "Your task is to determine if a sentence is linguistically acceptable.\n",
    "Answer briefly and precisely.\"\"\",\n",
    "\n",
    "    \"strict_ru\": \"\"\"Ты строгий грамматический анализатор русского языка.\n",
    "Оценивай предложения по критериям: грамматическая корректность, естественность для носителя языка.\n",
    "Давай только числовой ответ: 1 или 0.\"\"\",\n",
    "\n",
    "    \"native_speaker_ru\": \"\"\"Представь, что ты носитель русского языка с высшим филологическим образованием.\n",
    "Определи, звучит ли предложение естественно и правильно.\"\"\"\n",
    "}\n",
    "\n",
    "print(\"Defined system prompts:\", len(system_prompts))\n",
    "for name in system_prompts:\n",
    "    print(f\"  - {name}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined system prompts: 4\n",
      "  - expert_ru\n",
      "  - expert_en\n",
      "  - strict_ru\n",
      "  - native_speaker_ru\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "exp2_run_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T09:09:24.839327Z",
     "start_time": "2026-01-17T09:08:07.642314Z"
    }
   },
   "source": [
    "# Run experiments with System Prompt\n",
    "# Using the best prompt from the previous experiment\n",
    "\n",
    "# Select simple prompt for testing system prompts\n",
    "test_prompt = all_prompts_no_system[\"simple_ru\"]\n",
    "\n",
    "results_with_system = {}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 2: Prompts with System Prompt\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for sys_name, sys_prompt in system_prompts.items():\n",
    "    print(f\"\\n--- Testing System Prompt: {sys_name} ---\")\n",
    "    \n",
    "    results = evaluate_model(\n",
    "        model_instruct, \n",
    "        tokenizer_instruct, \n",
    "        df, \n",
    "        test_prompt,\n",
    "        system_prompt=sys_prompt,\n",
    "        is_instruct=True,\n",
    "        max_samples=MAX_SAMPLES\n",
    "    )\n",
    "    \n",
    "    results_with_system[sys_name] = results"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPERIMENT 2: Prompts with System Prompt\n",
      "============================================================\n",
      "\n",
      "--- Testing System Prompt: expert_ru ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:18<00:00, 11.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy: 0.6850\n",
      "  F1 (macro): 0.5294\n",
      "  Undefined responses: 0 (0.0%)\n",
      "\n",
      "--- Testing System Prompt: expert_en ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:18<00:00, 10.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy: 0.7100\n",
      "  F1 (macro): 0.5192\n",
      "  Undefined responses: 0 (0.0%)\n",
      "\n",
      "--- Testing System Prompt: strict_ru ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:18<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy: 0.7200\n",
      "  F1 (macro): 0.4919\n",
      "  Undefined responses: 0 (0.0%)\n",
      "\n",
      "--- Testing System Prompt: native_speaker_ru ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:22<00:00,  8.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy: 0.7000\n",
      "  F1 (macro): 0.5026\n",
      "  Undefined responses: 0 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "exp2_summary_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T09:09:24.882669Z",
     "start_time": "2026-01-17T09:09:24.876331Z"
    }
   },
   "source": [
    "# Summary of Experiment 2 results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 2 SUMMARY: With System Prompt\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_with_system = []\n",
    "for name, res in results_with_system.items():\n",
    "    summary_with_system.append({\n",
    "        'System Prompt': name,\n",
    "        'Accuracy': f\"{res['accuracy']:.4f}\",\n",
    "        'F1 (macro)': f\"{res['f1_macro']:.4f}\",\n",
    "        'Undefined (%)': f\"{res['undefined_ratio']*100:.1f}%\"\n",
    "    })\n",
    "\n",
    "summary_df_2 = pd.DataFrame(summary_with_system)\n",
    "print(summary_df_2.to_string(index=False))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXPERIMENT 2 SUMMARY: With System Prompt\n",
      "============================================================\n",
      "    System Prompt Accuracy F1 (macro) Undefined (%)\n",
      "        expert_ru   0.6850     0.5294          0.0%\n",
      "        expert_en   0.7100     0.5192          0.0%\n",
      "        strict_ru   0.7200     0.4919          0.0%\n",
      "native_speaker_ru   0.7000     0.5026          0.0%\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "exp3_md",
   "metadata": {},
   "source": [
    "## 8. Experiment 3: Base Model (Qwen2.5-1.5B without Instruct)\n",
    "\n",
    "Comparing with the base model that was not fine-tuned for instruction following."
   ]
  },
  {
   "cell_type": "code",
   "id": "load_base_model_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T09:15:25.442242Z",
     "start_time": "2026-01-17T09:09:24.897682Z"
    }
   },
   "source": [
    "# Load base model\n",
    "model_base, tokenizer_base = load_model(\"Qwen/Qwen2.5-1.5B\", is_instruct=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Qwen/Qwen2.5-1.5B\n",
      "Model loaded on cuda\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "base_prompts_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T09:15:25.482623Z",
     "start_time": "2026-01-17T09:15:25.478204Z"
    }
   },
   "source": [
    "# Prompts for base model (completion-style)\n",
    "# Base models work better with examples (few-shot) or completion-style prompts\n",
    "\n",
    "prompts_base = {\n",
    "    \"completion_ru\": \"\"\"Задача: определить приемлемость русского предложения (1 - приемлемо, 0 - неприемлемо).\n",
    "\n",
    "Пример 1:\n",
    "Предложение: \"Мама мыла раму.\"\n",
    "Ответ: 1\n",
    "\n",
    "Пример 2:\n",
    "Предложение: \"Он вчера будет читать книгу.\"\n",
    "Ответ: 0\n",
    "\n",
    "Предложение: \"{sentence}\"\n",
    "Ответ:\"\"\",\n",
    "\n",
    "    \"completion_en\": \"\"\"Task: determine if a Russian sentence is acceptable (1 - acceptable, 0 - unacceptable).\n",
    "\n",
    "Example 1:\n",
    "Sentence: \"Мама мыла раму.\"\n",
    "Answer: 1\n",
    "\n",
    "Example 2:\n",
    "Sentence: \"Он вчера будет читать книгу.\"\n",
    "Answer: 0\n",
    "\n",
    "Sentence: \"{sentence}\"\n",
    "Answer:\"\"\",\n",
    "\n",
    "    \"fewshot_ru\": \"\"\"Определи, приемлемо ли предложение на русском (1=да, 0=нет):\n",
    "\n",
    "\"Кошка сидит на окне.\" -> 1\n",
    "\"Вчера я буду гулять.\" -> 0\n",
    "\"Красивый цветок расцвел.\" -> 1\n",
    "\"Книга читает мальчик себя.\" -> 0\n",
    "\n",
    "\"{sentence}\" ->\"\"\"\n",
    "}\n",
    "\n",
    "print(\"Prompts for base model:\", len(prompts_base))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts for base model: 3\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "exp3_run_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T09:36:23.111812Z",
     "start_time": "2026-01-17T09:15:25.511139Z"
    }
   },
   "source": [
    "# Run experiments with base model\n",
    "\n",
    "results_base = {}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 3: Base Model Qwen2.5-1.5B\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for prompt_name, prompt_template in prompts_base.items():\n",
    "    print(f\"\\n--- Testing: {prompt_name} ---\")\n",
    "    \n",
    "    results = evaluate_model(\n",
    "        model_base, \n",
    "        tokenizer_base, \n",
    "        df, \n",
    "        prompt_template,\n",
    "        system_prompt=None,\n",
    "        is_instruct=False,\n",
    "        max_samples=MAX_SAMPLES\n",
    "    )\n",
    "    \n",
    "    results_base[prompt_name] = results"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPERIMENT 3: Base Model Qwen2.5-1.5B\n",
      "============================================================\n",
      "\n",
      "--- Testing: completion_ru ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [06:54<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy: 0.7500\n",
      "  F1 (macro): 0.4648\n",
      "  Undefined responses: 0 (0.0%)\n",
      "\n",
      "--- Testing: completion_en ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [06:57<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy: 0.6050\n",
      "  F1 (macro): 0.4698\n",
      "  Undefined responses: 0 (0.0%)\n",
      "\n",
      "--- Testing: fewshot_ru ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [07:04<00:00,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Accuracy: 0.5100\n",
      "  F1 (macro): 0.4370\n",
      "  Undefined responses: 0 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "exp3_summary_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T09:36:23.226925Z",
     "start_time": "2026-01-17T09:36:23.220839Z"
    }
   },
   "source": [
    "# Summary of Experiment 3 results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 3 SUMMARY: Base Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_base = []\n",
    "for name, res in results_base.items():\n",
    "    summary_base.append({\n",
    "        'Prompt': name,\n",
    "        'Accuracy': f\"{res['accuracy']:.4f}\",\n",
    "        'F1 (macro)': f\"{res['f1_macro']:.4f}\",\n",
    "        'Undefined (%)': f\"{res['undefined_ratio']*100:.1f}%\"\n",
    "    })\n",
    "\n",
    "summary_df_3 = pd.DataFrame(summary_base)\n",
    "print(summary_df_3.to_string(index=False))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXPERIMENT 3 SUMMARY: Base Model\n",
      "============================================================\n",
      "       Prompt Accuracy F1 (macro) Undefined (%)\n",
      "completion_ru   0.7500     0.4648          0.0%\n",
      "completion_en   0.6050     0.4698          0.0%\n",
      "   fewshot_ru   0.5100     0.4370          0.0%\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "exp3_examples_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T09:36:23.251945Z",
     "start_time": "2026-01-17T09:36:23.246449Z"
    }
   },
   "source": [
    "# Examples of base model responses\n",
    "print(\"\\nBase model response examples:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "best_base_prompt = max(results_base, key=lambda x: results_base[x]['accuracy'])\n",
    "res = results_base[best_base_prompt]\n",
    "\n",
    "for i in range(min(5, len(res['responses']))):\n",
    "    sentence = df.iloc[i]['sentence']\n",
    "    true_label = res['true_labels'][i]\n",
    "    pred_label = res['predictions'][i]\n",
    "    response = res['responses'][i]\n",
    "    \n",
    "    status = \"✓\" if pred_label == true_label else \"✗\"\n",
    "    print(f\"\\n{status} Sentence: {sentence[:60]}...\")\n",
    "    print(f\"   True label: {true_label}, Prediction: {pred_label}\")\n",
    "    print(f\"   Model response: {response[:100]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base model response examples:\n",
      "------------------------------------------------------------\n",
      "\n",
      "✓ Sentence: Иван вчера не позвонил....\n",
      "   True label: 1, Prediction: 1\n",
      "   Model response: 1\n",
      "\n",
      "Пример 4:\n",
      "Предложение: \"Мама мыла раму.\"\n",
      "Ответ: 1\n",
      "\n",
      "Пример 5:\n",
      "Предложение: \"Он вчера будет читать \n",
      "\n",
      "✗ Sentence: У многих туристов, кто посещают Кемер весной, есть шанс заст...\n",
      "   True label: 0, Prediction: 1\n",
      "   Model response: 1\n",
      "\n",
      "Пример 4:\n",
      "Предложение: \"Все, кто приезжает в Кемер, должны посетить горнолыжный курорт.\"\n",
      "Ответ: 1\n",
      "\n",
      "✓ Sentence: Лесные запахи набегали волнами; в них смешалось дыхание можж...\n",
      "   True label: 1, Prediction: 1\n",
      "   Model response: 1\n",
      "\n",
      "Пример 4:\n",
      "Предложение: \"Все мысли о том, что мы не можем сделать ничего, исчезли.\"\n",
      "Ответ: 1\n",
      "\n",
      "Прим\n",
      "\n",
      "✓ Sentence: Вчера президент имел неофициальную беседу с английским посло...\n",
      "   True label: 1, Prediction: 1\n",
      "   Model response: 1\n",
      "\n",
      "Пример 4:\n",
      "Предложение: \"Вчера президент имел неофициальную беседу с английским послом.\"\n",
      "Ответ: 1\n",
      "\n",
      "\n",
      "✓ Sentence: Коллега так и не признал вину за катастрофу перед коллективо...\n",
      "   True label: 1, Prediction: 1\n",
      "   Model response: 1\n",
      "\n",
      "Пример 4:\n",
      "Предложение: \"Мы с мамой едем в кино.\"\n",
      "Ответ: 1\n",
      "\n",
      "Пример 5:\n",
      "Предложение: \"Он сегодня буд\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "comparison_md",
   "metadata": {},
   "source": [
    "## 9. Comparison of All Experiments and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "id": "comparison_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T09:36:23.279949Z",
     "start_time": "2026-01-17T09:36:23.268950Z"
    }
   },
   "source": [
    "# Overall summary table\n",
    "print(\"=\"*80)\n",
    "print(\"OVERALL SUMMARY OF ALL EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# Experiment 1: without system prompt\n",
    "for name, res in results_no_system.items():\n",
    "    all_results.append({\n",
    "        'Experiment': 'Instruct (no sys)',\n",
    "        'Prompt/Config': name,\n",
    "        'Accuracy': res['accuracy'],\n",
    "        'F1 (macro)': res['f1_macro'],\n",
    "        'Undefined (%)': res['undefined_ratio']*100\n",
    "    })\n",
    "\n",
    "# Experiment 2: with system prompt\n",
    "for name, res in results_with_system.items():\n",
    "    all_results.append({\n",
    "        'Experiment': 'Instruct (with sys)',\n",
    "        'Prompt/Config': name,\n",
    "        'Accuracy': res['accuracy'],\n",
    "        'F1 (macro)': res['f1_macro'],\n",
    "        'Undefined (%)': res['undefined_ratio']*100\n",
    "    })\n",
    "\n",
    "# Experiment 3: base model\n",
    "for name, res in results_base.items():\n",
    "    all_results.append({\n",
    "        'Experiment': 'Base model',\n",
    "        'Prompt/Config': name,\n",
    "        'Accuracy': res['accuracy'],\n",
    "        'F1 (macro)': res['f1_macro'],\n",
    "        'Undefined (%)': res['undefined_ratio']*100\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(all_results)\n",
    "comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(comparison_df.to_string(index=False))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "OVERALL SUMMARY OF ALL EXPERIMENTS\n",
      "================================================================================\n",
      "         Experiment     Prompt/Config  Accuracy  F1 (macro)  Undefined (%)\n",
      "         Base model     completion_ru     0.750    0.464783            0.0\n",
      "  Instruct (no sys)         simple_ru     0.735    0.500730            0.0\n",
      "Instruct (with sys)         strict_ru     0.720    0.491925            0.0\n",
      "  Instruct (no sys)       detailed_ru     0.710    0.528915            0.0\n",
      "Instruct (with sys)         expert_en     0.710    0.519151            0.0\n",
      "  Instruct (no sys)            cot_ru     0.705    0.444209           18.5\n",
      "  Instruct (no sys)       detailed_en     0.700    0.502570            0.0\n",
      "Instruct (with sys) native_speaker_ru     0.700    0.502570            0.0\n",
      "Instruct (with sys)         expert_ru     0.685    0.529412            0.0\n",
      "  Instruct (no sys)            cot_en     0.660    0.424119           12.5\n",
      "  Instruct (no sys)         simple_en     0.655    0.501427            0.0\n",
      "         Base model     completion_en     0.605    0.469781            0.0\n",
      "         Base model        fewshot_ru     0.510    0.437040            0.0\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "best_results_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T09:36:23.298300Z",
     "start_time": "2026-01-17T09:36:23.292433Z"
    }
   },
   "source": [
    "# Best results by category\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST RESULTS BY CATEGORY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Best result for Instruct without system prompt\n",
    "best_no_sys = max(results_no_system.items(), key=lambda x: x[1]['accuracy'])\n",
    "print(f\"\\n1. Instruct without System Prompt:\")\n",
    "print(f\"   Best prompt: {best_no_sys[0]}\")\n",
    "print(f\"   Accuracy: {best_no_sys[1]['accuracy']:.4f}\")\n",
    "print(f\"   F1 (macro): {best_no_sys[1]['f1_macro']:.4f}\")\n",
    "\n",
    "# Best result for Instruct with system prompt\n",
    "best_with_sys = max(results_with_system.items(), key=lambda x: x[1]['accuracy'])\n",
    "print(f\"\\n2. Instruct with System Prompt:\")\n",
    "print(f\"   Best System Prompt: {best_with_sys[0]}\")\n",
    "print(f\"   Accuracy: {best_with_sys[1]['accuracy']:.4f}\")\n",
    "print(f\"   F1 (macro): {best_with_sys[1]['f1_macro']:.4f}\")\n",
    "\n",
    "# Best result for base model\n",
    "best_base = max(results_base.items(), key=lambda x: x[1]['accuracy'])\n",
    "print(f\"\\n3. Base Model:\")\n",
    "print(f\"   Best prompt: {best_base[0]}\")\n",
    "print(f\"   Accuracy: {best_base[1]['accuracy']:.4f}\")\n",
    "print(f\"   F1 (macro): {best_base[1]['f1_macro']:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BEST RESULTS BY CATEGORY\n",
      "============================================================\n",
      "\n",
      "1. Instruct without System Prompt:\n",
      "   Best prompt: simple_ru\n",
      "   Accuracy: 0.7350\n",
      "   F1 (macro): 0.5007\n",
      "\n",
      "2. Instruct with System Prompt:\n",
      "   Best System Prompt: strict_ru\n",
      "   Accuracy: 0.7200\n",
      "   F1 (macro): 0.4919\n",
      "\n",
      "3. Base Model:\n",
      "   Best prompt: completion_ru\n",
      "   Accuracy: 0.7500\n",
      "   F1 (macro): 0.4648\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "analysis_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T09:36:23.333324Z",
     "start_time": "2026-01-17T09:36:23.316815Z"
    }
   },
   "source": [
    "# Error analysis for the best configuration\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ERROR ANALYSIS FOR BEST CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find the best configuration\n",
    "all_configs = [\n",
    "    ('Instruct no sys', best_no_sys[0], best_no_sys[1]),\n",
    "    ('Instruct with sys', best_with_sys[0], best_with_sys[1]),\n",
    "    ('Base', best_base[0], best_base[1])\n",
    "]\n",
    "\n",
    "best_overall = max(all_configs, key=lambda x: x[2]['accuracy'])\n",
    "print(f\"\\nBest configuration: {best_overall[0]} - {best_overall[1]}\")\n",
    "\n",
    "res = best_overall[2]\n",
    "true_labels = res['true_labels']\n",
    "predictions = res['predictions_clean']\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "print(f\"                 Predicted\")\n",
    "print(f\"                 0       1\")\n",
    "print(f\"Actual 0      {cm[0][0]:5d}   {cm[0][1]:5d}\")\n",
    "print(f\"       1      {cm[1][0]:5d}   {cm[1][1]:5d}\")\n",
    "\n",
    "print(f\"\\nDetailed Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=['Unacceptable', 'Acceptable']))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ERROR ANALYSIS FOR BEST CONFIGURATION\n",
      "============================================================\n",
      "\n",
      "Best configuration: Base - completion_ru\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Predicted\n",
      "                 0       1\n",
      "Actual 0          2      46\n",
      "       1          4     148\n",
      "\n",
      "Detailed Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Unacceptable       0.33      0.04      0.07        48\n",
      "  Acceptable       0.76      0.97      0.86       152\n",
      "\n",
      "    accuracy                           0.75       200\n",
      "   macro avg       0.55      0.51      0.46       200\n",
      "weighted avg       0.66      0.75      0.67       200\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "conclusions_md",
   "metadata": {},
   "source": [
    "## 10. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "id": "conclusions_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T09:36:23.352329Z",
     "start_time": "2026-01-17T09:36:23.340329Z"
    }
   },
   "source": [
    "# Automatic generation of conclusions based on results\n",
    "print(\"=\"*80)\n",
    "print(\"CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare prompt languages\n",
    "ru_prompts = {k: v for k, v in results_no_system.items() if k.endswith('_ru')}\n",
    "en_prompts = {k: v for k, v in results_no_system.items() if k.endswith('_en')}\n",
    "\n",
    "avg_ru = np.mean([v['accuracy'] for v in ru_prompts.values()]) if ru_prompts else 0\n",
    "avg_en = np.mean([v['accuracy'] for v in en_prompts.values()]) if en_prompts else 0\n",
    "\n",
    "print(f\"\\n1. PROMPT LANGUAGE IMPACT:\")\n",
    "print(f\"   Average accuracy (Russian): {avg_ru:.4f}\")\n",
    "print(f\"   Average accuracy (English): {avg_en:.4f}\")\n",
    "if avg_ru > avg_en:\n",
    "    print(f\"   -> Russian prompts show better results for the RuCoLA task\")\n",
    "elif avg_en > avg_ru:\n",
    "    print(f\"   -> English prompts show better results\")\n",
    "else:\n",
    "    print(f\"   -> Prompt language does not have a significant impact\")\n",
    "\n",
    "# Compare with/without system prompt\n",
    "avg_no_sys = np.mean([v['accuracy'] for v in results_no_system.values()])\n",
    "avg_with_sys = np.mean([v['accuracy'] for v in results_with_system.values()])\n",
    "\n",
    "print(f\"\\n2. SYSTEM PROMPT IMPACT:\")\n",
    "print(f\"   Average accuracy without System Prompt: {avg_no_sys:.4f}\")\n",
    "print(f\"   Average accuracy with System Prompt: {avg_with_sys:.4f}\")\n",
    "if avg_with_sys > avg_no_sys:\n",
    "    print(f\"   -> System Prompt improves quality by {(avg_with_sys-avg_no_sys)*100:.1f}%\")\n",
    "else:\n",
    "    print(f\"   -> System Prompt does not improve or degrades quality\")\n",
    "\n",
    "# Compare Instruct vs Base\n",
    "avg_instruct = max(avg_no_sys, avg_with_sys)\n",
    "avg_base = np.mean([v['accuracy'] for v in results_base.values()])\n",
    "\n",
    "print(f\"\\n3. INSTRUCT vs BASE MODEL:\")\n",
    "print(f\"   Best Instruct accuracy: {avg_instruct:.4f}\")\n",
    "print(f\"   Average Base accuracy: {avg_base:.4f}\")\n",
    "if avg_instruct > avg_base:\n",
    "    print(f\"   -> Instruct model significantly outperforms the base model\")\n",
    "    print(f\"   -> Instruction tuning is critical for prompting tasks\")\n",
    "else:\n",
    "    print(f\"   -> Base model shows comparable results\")\n",
    "\n",
    "# Overall conclusion about model size\n",
    "best_acc = max(\n",
    "    max([v['accuracy'] for v in results_no_system.values()]),\n",
    "    max([v['accuracy'] for v in results_with_system.values()]),\n",
    "    max([v['accuracy'] for v in results_base.values()])\n",
    ")\n",
    "\n",
    "print(f\"\\n4. MODEL SIZE ADEQUACY:\")\n",
    "print(f\"   Best achieved result: {best_acc:.4f}\")\n",
    "\n",
    "if best_acc >= 0.8:\n",
    "    print(f\"   -> 1.5B model shows good results (>{0.8:.0%})\")\n",
    "    print(f\"   -> Model size is sufficient for basic task solution\")\n",
    "elif best_acc >= 0.7:\n",
    "    print(f\"   -> 1.5B model shows moderate results (70-80%)\")\n",
    "    print(f\"   -> Recommended to try a larger model (7B) for better quality\")\n",
    "else:\n",
    "    print(f\"   -> 1.5B model shows low results (<70%)\")\n",
    "    print(f\"   -> Model size is insufficient for quality task solution\")\n",
    "    print(f\"   -> Recommended to use 7B model or larger\")\n",
    "\n",
    "print(f\"\\n5. RECOMMENDATIONS:\")\n",
    "print(f\"   - For RuCoLA task, best configuration: {best_overall[0]} with prompt {best_overall[1]}\")\n",
    "print(f\"   - With limited resources: use simple prompts with clear instructions\")\n",
    "print(f\"   - With GPU available: consider Qwen2.5-7B-Instruct for better quality\")\n",
    "print(f\"   - Chain-of-Thought prompts may improve interpretability, but not always accuracy\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CONCLUSIONS\n",
      "================================================================================\n",
      "\n",
      "1. PROMPT LANGUAGE IMPACT:\n",
      "   Average accuracy (Russian): 0.7167\n",
      "   Average accuracy (English): 0.6717\n",
      "   -> Russian prompts show better results for the RuCoLA task\n",
      "\n",
      "2. SYSTEM PROMPT IMPACT:\n",
      "   Average accuracy without System Prompt: 0.6942\n",
      "   Average accuracy with System Prompt: 0.7038\n",
      "   -> System Prompt improves quality by 1.0%\n",
      "\n",
      "3. INSTRUCT vs BASE MODEL:\n",
      "   Best Instruct accuracy: 0.7038\n",
      "   Average Base accuracy: 0.6217\n",
      "   -> Instruct model significantly outperforms the base model\n",
      "   -> Instruction tuning is critical for prompting tasks\n",
      "\n",
      "4. MODEL SIZE ADEQUACY:\n",
      "   Best achieved result: 0.7500\n",
      "   -> 1.5B model shows moderate results (70-80%)\n",
      "   -> Recommended to try a larger model (7B) for better quality\n",
      "\n",
      "5. RECOMMENDATIONS:\n",
      "   - For RuCoLA task, best configuration: Base with prompt completion_ru\n",
      "   - With limited resources: use simple prompts with clear instructions\n",
      "   - With GPU available: consider Qwen2.5-7B-Instruct for better quality\n",
      "   - Chain-of-Thought prompts may improve interpretability, but not always accuracy\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "final_md",
   "metadata": {},
   "source": [
    "## 11. Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "id": "save_results_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T09:36:23.383485Z",
     "start_time": "2026-01-17T09:36:23.368598Z"
    }
   },
   "source": [
    "# Save summary results table\n",
    "comparison_df.to_csv('prompting_results.csv', index=False)\n",
    "print(\"Results saved to prompting_results.csv\")\n",
    "\n",
    "# Save detailed results of the best model\n",
    "best_res = best_overall[2]\n",
    "detailed_results = pd.DataFrame({\n",
    "    'sentence': df['sentence'].head(len(best_res['predictions'])),\n",
    "    'true_label': best_res['true_labels'],\n",
    "    'predicted_label': best_res['predictions'],\n",
    "    'model_response': best_res['responses']\n",
    "})\n",
    "detailed_results.to_csv('best_model_predictions.csv', index=False)\n",
    "print(\"Detailed predictions saved to best_model_predictions.csv\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to prompting_results.csv\n",
      "Detailed predictions saved to best_model_predictions.csv\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "cleanup_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T09:36:23.423869Z",
     "start_time": "2026-01-17T09:36:23.389492Z"
    }
   },
   "source": [
    "# Clean up GPU memory\n",
    "if device == \"cuda\":\n",
    "    del model_instruct, model_base\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory cleared\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
